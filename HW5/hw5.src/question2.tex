\section{Experiments}\label{sec:q2}

For this question, you will have to implement and compare different
learning strategies: SVM, logistic regression (from your answer to
the previous question), the naive Bayes classifier, and a variant of
random forests that combines SVMs and decision trees.


\subsection{Algorithms to Compare}

\begin{enumerate}
\item~[15 points] \textbf{Support Vector Machine}

  Implement the simple stochastic sub-gradient descent version
  algorithm SVM as described in the class. Assume that the learning
  rate for the $t^{th}$ epoch is

  $$\gamma_t = \frac{\gamma_0}{1 + t}$$

  For this, and all subsequent implementations, you should choose an
  appropriate number of epochs and justify your choice. One way to
  select the number of epochs is to observe the value of the SVM
  objective over the epochs and stop if the change in the value is
  smaller than some small threshold. You do not have to use this
  strategy, but your report should specify the number of epochs you
  chose.

  \textbf{Hyper-parameters}: 
  \begin{enumerate}
  \item Initial learning rate: $\gamma_0\in\{10^1, 10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\}$
  \item The regularization/loss tradeoff parameter: $C\in \{10^1, 10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\}$
  \end{enumerate}
  \underline{Ans:} Implementation involved modifying the weights update method on the simple perceptron code. Rest all remaining the same. Cross validation was done on learning rate and balancer C, in a decreasing order. Prediction cross product * label $\ge$ 0 is labeled positive else negative. And if for any set of hyper parameter average is less than 1/3rd of max average then I skip the hyper parameter values.\\\\
  Cross Validation results: \\
  \hspace*{4mm}Learning rate 0.01, balancer 10\\
  \hspace*{4mm}Averages:\\
  \hspace*{8mm}F1 = 0.442605\\
  \hspace*{8mm}Precision = 0.685459\\
  \hspace*{8mm}Recall = 0.326856\\
  \hspace*{8mm}Accuracy = 81.86\%\\
  Test Run Results using learning rate 0.01 and load balancer 10:\\
  \hspace*{8mm}F1 = 0.449118\\
  \hspace*{8mm}Precision = 0.722388\\
  \hspace*{8mm}Recall = 0.325853\\
  \hspace*{8mm}Accuracy = 82.19\%\\
  
  
\item~[15 points] \textbf{Logistic regression}

  Implement the Logistic Regression learner based on your algorithm in
  the Question~\ref{sec:q1}.

  \textbf{Hyper-parameters}: 
  \begin{enumerate}
  \item Initial learning rate: $\gamma_0\in\{10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$
  \item Tradeoff: $\sigma^2\in \{10^{-1}, 10^0, 10^{1}, 10^{2}, 10^{3}, 10^{4}\}$
  \end{enumerate}
  Implementation based on Perceptron. Weight vector updates are based on 1b - stochastic gradient descent. Updates are made when dot product of weight and example * label is less than or equal to 0. Initial weight vector is assignment to 0, but there was no difference when assigned with random weights between -1 to 1.
  Best Parameters:\\
  \hspace*{4mm}Learning rate 0.001
  \hspace*{4mm}sigma squared 100\\
  \hspace*{8mm}Cross Validation:\\
  \hspace*{12mm}Avg. F1 = 0.403011\\
  \hspace*{12mm}Avg. Precision = 0.391119\\
  \hspace*{12mm}Avg. Recall = 0.428492\\
  \hspace*{12mm}Avg. Accuracy = 71.93\%\\
  \hspace*{8mm}Test:\\
  \hspace*{12mm}F1 = 0.381619\\
  \hspace*{12mm}Precision = 0.475553\\
  \hspace*{12mm}Recall = 0.318671\\
  \hspace*{12mm}Accuracy on Test = 76.99\%\\
  
\item~[15 points] \textbf{Naive Bayes}

  Implement the simple Naive Bayes learner. You need to count the
  features to get the likelihoods one at a time. To get the prior, you
  will need to count the number of examples in each class.  

  For every feature $x_i$, you should estimate its likelihood of
  taking a value for a given label $y$ (which is either $+$ or $-$)
  as:
  %
  \begin{equation*}
    P(x_i \vert y) = \frac{Count(x_i, y)+\lambda}{Count(y)+S_i\lambda}.
  \end{equation*}

  Here, $S_i$ is the number of all possible values that $x_i$ can take
  in the data. (In the data provided, each feature is binary, which
  should simplify your implementation a lot.)

  The hyper-parameter $\lambda$ is a smoothing term. In example we saw
  in class, we set $\lambda= 1$.  But, in this experiment, you should
  choose the best $\lambda$ based on cross-validation.

  \textbf{Hyper-parameter}: Smoothing term: $\lambda \in \{2, 1.5, 1.0, 0.5\}$\\\\
  \underline{Ans:} Implemented the algorithm completely from the scratch. Training is basically finding probability for every value which every feature could take for every labels in the data. I used logarithm of the probabilities to help with floating point precision issues on multiplication. With log probabilities we can just add the values and predict. Prediction is done for every label and highest probability label is used as the prediction. To calculate the probability we add log of raw prob of the label and then add rest of the log probability based on the value of the features present. Train phase saves all the log probabilities in a dictionary\\
  Cross validation results:\\
  \hspace*{4mm}Best smoothing term = 2 \\
  \hspace*{4mm}average F1 = 0.508315 \\
  \hspace*{4mm}average Precision = 0.497278 \\
  \hspace*{4mm}average Recall = 0.520284\\
  \hspace*{4mm}average accuracy = 77.82\%\\
  Test run results using smoothing term = 2\\
  \hspace*{4mm}F1 = 0.526526\\
  \hspace*{4mm}Precision = 0.520861\\
  \hspace*{4mm}Recall = 0.532316\\
  \hspace*{4mm}Accuracy = 78.67\%
  

\item~[25 points] \textbf{SVM over trees}

  In class we have learned how the bagging and random forest
  algorithms work.  In this setting, you are going to build a
  different ensemble over depth-limited decision trees that are
  learned using the ID3 algorithm.

  First, using the training set, you need to build $200$ decision
  trees. To construct a decision tree, you need to sample $10\%$ of
  the examples {\em with replacement} from the training set
  (i.e. 2000 examples), and use this subset to train your decision
  tree with a depth limit $d$. Repeating this $200$ times will get
  you $200$ trees.

  Usually, the final prediction will be voted on by these
  trees. However, we would like to train an SVM to combine these
  predictions. To do so, you should treat the $200$ trees as a
  feature transformation and construct a new dataset by applying the
  transformation. That is, suppose your trees were
  $tree_1, tree_2, \cdots, tree_{200}$. Each of these are functions
  that can predict a label for an example that is either $-1$ or
  $+1$. Instead of just predicting the label, treat them as a
  feature transformation $\phi(x)$ that is defined as:

  
  $$\phi(x) = [tree_1(x), tree_2(x), \cdots, tree_{N} (x)]$$  

  In other words, you will build an $N$ dimensional vector
  consisting of the prediction (1 or -1) of each tree that you
  created. Thus, you have a {\em learned} feature transformation.

  Now, you can train an SVM on these transformed features. (Don't
  forget to transform the test set before making your final
  evaluations.)

  \textbf{Hyper-parameters}:
  \begin{enumerate}
  \item Initial learning rate $\gamma_0 \in\{10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$
  \item Tradeoff $C \in \{10^1, 10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$
  \item Depth: $d \in \{10, 20, 30\}$
  \end{enumerate}
\underline{Ans:} For fixing the cross validation I assumed few things.\\
Due to time constraints I did cross validation with 20 trees. I built 20 decision trees sampling 10\% of the training set. Used these trees to transform features on both the cross validation training and testing set.\\\\
After generating all the cross validation training and testing sets, used it on an svm to cross validate the hyper parameters of learning rate and load balancer.\\
Initial run with 20 trees for cross validation gave best parameters as:\\
\hspace*{4mm}limiting depth = 10 \\
\hspace*{4mm}learning rate = 1\\
\hspace*{4mm}balancer = 0.1\\
\hspace*{2mm}With averages as follows:\\
\hspace*{4mm}F1 = 0.394505\\
\hspace*{4mm}Precision = 0.677340\\
\hspace*{4mm}Recall = 0.278241\\
\hspace*{4mm}Accuracy = 81.18\%\\
Final Run was done using learning rate 1 and load balancer as 0.1 with 200 trees on the test set.\\
Results are as follows:\\
\hspace*{4mm}limiting depth = 10 \\
\hspace*{4mm}learning rate = 1\\
\hspace*{4mm}balancer = 0.1\\
\hspace*{2mm}With averages as follows:\\
\hspace*{4mm}F1 = 0.4\\
\hspace*{4mm}Precision = 0.719953\\
\hspace*{4mm}Recall = 0.27693\\
\hspace*{4mm}Accuracy = 81.49\%\\\\
\underline{Observation:} For this data set the F1 averages are very close to one another except for few hyper parameter values like learning rate 1 balancer 10. So every time it is executed we get a different value for learning rate and balancer, but the values for different stats remains almost the same.
\end{enumerate}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw5"
%%% End:
